{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bic1FsJmYCVQ",
        "outputId": "d7181907-6496-4bbc-e1c9-69b0cef4372e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyautogen[teachable]\n",
            "  Downloading pyautogen-0.2.0-py3-none-any.whl (114 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/114.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m112.6/114.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.4/114.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.3.5-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cohere\n",
            "  Downloading cohere-4.36-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from pyautogen[teachable]) (5.6.3)\n",
            "Collecting flaml (from pyautogen[teachable])\n",
            "  Downloading FLAML-2.1.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.2/295.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv (from pyautogen[teachable])\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from pyautogen[teachable]) (2.3.0)\n",
            "Collecting chromadb (from pyautogen[teachable])\n",
            "  Downloading chromadb-0.4.18-py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.4/502.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.8.6)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro==1.8.2 (from cohere)\n",
            "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb->pyautogen[teachable])\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb->pyautogen[teachable])\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb->pyautogen[teachable])\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb->pyautogen[teachable])\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb->pyautogen[teachable])\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb->pyautogen[teachable])\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb->pyautogen[teachable])\n",
            "  Downloading opentelemetry_api-1.21.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb->pyautogen[teachable])\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb->pyautogen[teachable])\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb->pyautogen[teachable])\n",
            "  Downloading opentelemetry_sdk-1.21.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (0.15.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb->pyautogen[teachable])\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb->pyautogen[teachable])\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (1.59.2)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb->pyautogen[teachable])\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb->pyautogen[teachable])\n",
            "  Downloading kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb->pyautogen[teachable])\n",
            "  Downloading mmh3-4.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (1.23.5)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb->pyautogen[teachable])\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions<5,>=4.5 (from openai)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (1.6.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (3.2.2)\n",
            "Collecting urllib3<3,>=1.26 (from cohere)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb->pyautogen[teachable])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->pyautogen[teachable]) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->pyautogen[teachable]) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->pyautogen[teachable]) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->pyautogen[teachable]) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb->pyautogen[teachable])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->pyautogen[teachable]) (1.61.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->pyautogen[teachable])\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->pyautogen[teachable])\n",
            "  Downloading opentelemetry_proto-1.21.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[teachable])\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl (13 kB)\n",
            "Collecting opentelemetry-instrumentation==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[teachable])\n",
            "  Downloading opentelemetry_instrumentation-0.42b0-py3-none-any.whl (25 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[teachable])\n",
            "  Downloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[teachable])\n",
            "  Downloading opentelemetry_util_http-0.42b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[teachable]) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[teachable]) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[teachable])\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb->pyautogen[teachable])\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb->pyautogen[teachable]) (0.19.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb->pyautogen[teachable]) (8.1.7)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[teachable])\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[teachable])\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[teachable])\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[teachable])\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->pyautogen[teachable]) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->pyautogen[teachable]) (2023.6.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb->pyautogen[teachable])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb->pyautogen[teachable]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (0.5.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=9a2a77914f85aae8b1fe28ecfc1379a5aaa30cde60ce176c74a036bd3e40170d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, urllib3, typing-extensions, python-dotenv, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, humanfriendly, httptools, h11, flaml, fastavro, deprecated, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, httpcore, coloredlogs, asgiref, tiktoken, posthog, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, httpx, fastapi, cohere, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, openai, kubernetes, pyautogen, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.7.2 backoff-2.2.1 bcrypt-4.0.1 chroma-hnswlib-0.7.3 chromadb-0.4.18 cohere-4.36 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.104.1 fastavro-1.8.2 flaml-2.1.1 h11-0.14.0 httpcore-1.0.2 httptools-0.6.1 httpx-0.25.2 humanfriendly-10.0 kubernetes-28.1.0 mmh3-4.0.1 monotonic-1.6 onnxruntime-1.16.3 openai-1.3.5 opentelemetry-api-1.21.0 opentelemetry-exporter-otlp-proto-common-1.21.0 opentelemetry-exporter-otlp-proto-grpc-1.21.0 opentelemetry-instrumentation-0.42b0 opentelemetry-instrumentation-asgi-0.42b0 opentelemetry-instrumentation-fastapi-0.42b0 opentelemetry-proto-1.21.0 opentelemetry-sdk-1.21.0 opentelemetry-semantic-conventions-0.42b0 opentelemetry-util-http-0.42b0 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 pyautogen-0.2.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 tiktoken-0.5.1 typing-extensions-4.8.0 urllib3-1.26.18 uvicorn-0.24.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install \"pyautogen[teachable]\" openai cohere tiktoken --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import autogen\n",
        "\n",
        "config_list = [\n",
        "    {\n",
        "        \"model\": \"gpt-3.5-turbo\",\n",
        "        \"api_key\": \"api-key\"\n",
        "    }\n",
        "]\n",
        "llm_config = {\n",
        "    \"seed\": 42,\n",
        "    \"config_list\": config_list,\n",
        "    \"temperature\": 0\n",
        "}"
      ],
      "metadata": {
        "id": "a9v9XzyrYPDt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen.agentchat.contrib.teachable_agent import TeachableAgent\n",
        "\n",
        "teachable_agent  = TeachableAgent(\n",
        "    name = \"teachableagent\",\n",
        "    llm_config = llm_config,\n",
        "    teach_config = {\n",
        "        \"verbosity\": 0,  # 0 for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
        "        \"reset_db\": True,  # Set to True to start over with an empty database.\n",
        "        \"path_to_db_dir\": \"/content/sample_data\",  # Path to the directory where the database will be stored.\n",
        "        \"recall_threshold\": 1.5,  # Higher numbers allow more (but less relevant) memos to be recalled.\n",
        "    }\n",
        ")\n",
        "\n",
        "user = autogen.UserProxyAgent(\n",
        "    name = \"user\",\n",
        "    human_input_mode = \"NEVER\",\n",
        "    is_termination_msg = lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
        "    max_consecutive_auto_reply = 0,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZZk1eT7Y0oK",
        "outputId": "6957c1fa-3367-4f96-9011-f6374e6f178e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CLEARING MEMORY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What is the Vicuna model?\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gMjNR29ZkQp",
        "outputId": "b0ae21e8-6803-4e09-e70b-1b80743a0c01"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "What is the Vicuna model?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "I'm sorry, but I don't have any information about a specific \"Vicuna model.\" Could you please provide more context or clarify what you are referring to?\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Vicuna is a 13B-parameter language model released by Meta.\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5QSqpwxZrrj",
        "outputId": "c662f5f4-e3e4-4819-87b4-bea026455bfa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "Vicuna is a 13B-parameter language model released by Meta.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "Thank you for providing more information. I apologize for the confusion earlier. As of my last update, I don't have specific knowledge about the Vicuna model released by Meta. However, it's possible that the Vicuna model is a language model developed by Meta, which could potentially have 13 billion parameters. Language models like these are designed to generate human-like text based on the input they receive. If there are any specific questions or topics you'd like assistance with, feel free to ask!\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What is the Orca model?\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1YjkPy-ZyMf",
        "outputId": "8dfaeb40-0013-4c1d-f1e8-f4aedebe42ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "What is the Orca model?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "As of my last update, I don't have specific information about an \"Orca\" model. It's possible that the Orca model you are referring to is a language model developed by a particular organization or research group. Language models are designed to understand and generate human-like text based on the input they receive. If you have any additional context or details about the Orca model, I might be able to provide more specific information.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bqBaOAOZ2KZ",
        "outputId": "8580e7f6-e9d5-44c0-b03f-bb3e9d0e5c72"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "Thank you for providing the additional information. As of my last update, I don't have specific knowledge about the Orca model released by Microsoft. However, based on the details you've provided, it seems that the Orca model is a language model developed by Microsoft with 13 billion parameters. It's interesting to note that you mentioned it outperforms the Vicuna model on most tasks. Language models are continually evolving, and different models may excel in different areas based on their training data and architecture. If there's anything else you'd like to know or discuss, feel free to ask!\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use it at the end of each chat\n",
        "teachable_agent.learn_from_user_feedback()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYyZTvQZZ4fT",
        "outputId": "951cc767-b698-4f61-db1c-0ffda4a45ec8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "REVIEWING CHAT FOR USER TEACHINGS TO REMEMBER\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:00<00:00, 90.6MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"How does the Vicuna model compare to the Orca model?\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adKdbcqXZ7jB",
        "outputId": "04d079c6-a068-4017-8d0a-b143839557d8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "How does the Vicuna model compare to the Orca model?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "The Vicuna model is a language model developed by OpenAI. It is a powerful model, but it has been outperformed by the Orca model, which is a 13B-parameter language model released by Microsoft. The Orca model generally performs better than Vicuna on most tasks.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Please summarize this abstract.\n",
        "\n",
        "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
        "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
        "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
        "\"\"\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsSFO1nEaI8f",
        "outputId": "0c8221b9-5768-4381-ec61-52ab823a5dfa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "Please summarize this abstract.\n",
            "\n",
            "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
            "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
            "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "The abstract summarizes a research paper titled \"AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation.\" The paper introduces AutoGen, an open-source framework that allows developers to build LLM (Language and Learning Model) applications using multiple conversational agents. These agents can interact with each other to complete tasks and can be customized to operate in different modes, combining LLMs, human inputs, and tools. The framework also enables developers to define flexible conversation patterns using natural language and computer code. AutoGen serves as a generic infrastructure for building applications with various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in different domains such as mathematics, coding, question answering, operations research, online decision-making, and entertainment.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Please summarize this abstract.\n",
        "When I'm summarizing an abstract, I try to make the summary contain just three short bullet points:  the title, the innovation, and the key empirical results.\n",
        "\n",
        "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
        "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
        "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
        "\"\"\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPQc5wfiaPOJ",
        "outputId": "61801b6d-2211-4f74-8f09-a6b49ae09e27"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "Please summarize this abstract. \n",
            "When I'm summarizing an abstract, I try to make the summary contain just three short bullet points:  the title, the innovation, and the key empirical results.\n",
            "\n",
            "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
            "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
            "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "- Title: AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
            "- Innovation: AutoGen is an open-source framework that allows developers to build LLM applications using multiple conversational agents that can interact with each other to complete tasks.\n",
            "- Key empirical results: Empirical studies show that AutoGen is effective in various example applications, including mathematics, coding, question answering, operations research, online decision-making, and entertainment.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teachable_agent.learn_from_user_feedback()\n",
        "\n",
        "text = \"\"\"Please summarize this abstract.\n",
        "\n",
        "Sparks of Artificial General Intelligence: Early experiments with GPT-4\n",
        "Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang\n",
        "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.\"\"\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh6sdp_vaRwB",
        "outputId": "37901259-82be-485c-b100-dcc6db704b3b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "REVIEWING CHAT FOR USER TEACHINGS TO REMEMBER\n",
            "user (to teachableagent):\n",
            "\n",
            "Please summarize this abstract.\n",
            "\n",
            "Sparks of Artificial General Intelligence: Early experiments with GPT-4\n",
            "Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang\n",
            "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4\n",
            "\n",
            "Innovation: GPT-4, an advanced language model developed by OpenAI, demonstrates more general intelligence than previous AI models, including ChatGPT and Google's PaLM. It can solve complex tasks across various domains without specific prompts.\n",
            "\n",
            "Key empirical results: GPT-4's performance is remarkably close to human-level in tasks spanning mathematics, coding, vision, medicine, law, psychology, and more. It surpasses prior models like ChatGPT. The study explores GPT-4's limitations and discusses the challenges of advancing towards more comprehensive versions of artificial general intelligence.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Consider the identity:\n",
        "9 * 4 + 6 * 6 = 72\n",
        "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
        "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
        "\"\"\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Rr2azv9aZN5",
        "outputId": "d02cee1a-0277-435f-f7a4-64107980533f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "Consider the identity:  \n",
            "9 * 4 + 6 * 6 = 72\n",
            "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
            "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "To modify the equation 9 * 4 + 6 * 6 = 72 so that the right-hand side becomes 99, we can follow these steps:\n",
            "\n",
            "1. Identify the integer that can be modified to achieve the desired result. In this case, the integer that can be modified is 9.\n",
            "\n",
            "2. Determine the new value for the modified integer. To make the right-hand side of the equation equal to 99, we need to increase the value of 9. Let's increase it by 3, making it 12.\n",
            "\n",
            "3. Rewrite the equation with the modified integer. The modified equation becomes: 12 * 4 + 6 * 6 = 72.\n",
            "\n",
            "Therefore, the solution is: 12 * 4 + 6 * 6.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Consider the identity:\n",
        "9 * 4 + 6 * 6 = 72\n",
        "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
        "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
        "\n",
        "Here's some advice that may help:\n",
        "1. Let E denote the original number on the right.\n",
        "2. Let F denote the final number on the right.\n",
        "3. Calculate the difference between the two, G = F - E.\n",
        "4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.\n",
        "5. Calculate J = G / H. This is the number of times that H divides into G.\n",
        "6. Verify that J is an integer, and that H * J = G.\n",
        "7. Find the number on the left which is multiplied by H, and call it K.\n",
        "8. Change K to K + J.\n",
        "9. Recompute the value on the left, and verify that it equals F.\n",
        "Finally, write down your solution as: \"The solution is: A * B + C * D\".\n",
        "\"\"\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcGWdSPtabo-",
        "outputId": "d9f8b26a-9340-4296-ed8d-8a99372af553"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "Consider the identity:  \n",
            "9 * 4 + 6 * 6 = 72\n",
            "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
            "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
            "\n",
            "Here's some advice that may help:\n",
            "1. Let E denote the original number on the right.\n",
            "2. Let F denote the final number on the right.\n",
            "3. Calculate the difference between the two, G = F - E.\n",
            "4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.\n",
            "5. Calculate J = G / H. This is the number of times that H divides into G.\n",
            "6. Verify that J is an integer, and that H * J = G.\n",
            "7. Find the number on the left which is multiplied by H, and call it K.\n",
            "8. Change K to K + J.\n",
            "9. Recompute the value on the left, and verify that it equals F.\n",
            "Finally, write down your solution as: \"The solution is: A * B + C * D\".\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "To modify the equation 9 * 4 + 6 * 6 = 72 so that the right-hand side becomes 99, we can follow these steps:\n",
            "\n",
            "1. Let E denote the original number on the right, which is 72 in this case.\n",
            "2. Let F denote the final number on the right, which is 99 in this case.\n",
            "3. Calculate the difference between the two, G = F - E. In this case, G = 99 - 72 = 27.\n",
            "4. Examine the numbers on the left one by one until finding one that divides evenly into G. In this case, 9 divides evenly into 27.\n",
            "5. Calculate J = G / H, where H is the number that divides evenly into G. In this case, J = 27 / 9 = 3.\n",
            "6. Verify that J is an integer, and that H * J = G. In this case, J is an integer and 9 * 3 = 27.\n",
            "7. Find the number on the left which is multiplied by H, and call it K. In this case, K is 9.\n",
            "8. Change K to K + J. In this case, K + J = 9 + 3 = 12.\n",
            "9. Recompute the value on the left, and verify that it equals F. In this case, 12 * 4 + 6 * 6 = 48 + 36 = 84, which is not equal to 99.\n",
            "\n",
            "Since the recomputed value on the left does not equal F, it is not possible to modify exactly one integer on the left-hand side of the equation to make the right-hand side become 99.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teachable_agent.learn_from_user_feedback()\n",
        "\n",
        "text = \"\"\"Consider the identity:\n",
        "9 * 4 + 6 * 6 = 72\n",
        "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
        "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
        "\"\"\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzYl-_CzadyZ",
        "outputId": "8931a9b9-d6ad-4eff-fab0-e82ee726a8b7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "REVIEWING CHAT FOR USER TEACHINGS TO REMEMBER\n",
            "user (to teachableagent):\n",
            "\n",
            "Consider the identity:  \n",
            "9 * 4 + 6 * 6 = 72\n",
            "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
            "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "The solution is: 9 * 4 + 9 * 6\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Consider the identity:\n",
        "8 * 3 + 7 * 9 = 87\n",
        "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?\n",
        "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
        "\"\"\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6dWGlO3ai2V",
        "outputId": "b392e280-6b7c-4faf-c03f-72a3a6645ea9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "Consider the identity:  \n",
            "8 * 3 + 7 * 9 = 87\n",
            "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?\n",
            "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "I apologize for the confusion. Let's go through the steps again to find the correct solution.\n",
            "\n",
            "1. Let E denote the original number on the right, which is 87.\n",
            "2. Let F denote the final number on the right, which is 59.\n",
            "3. Calculate the difference between the two, G = F - E, which is -28.\n",
            "4. Examine the numbers on the left one by one until finding one that divides evenly into G. In this case, 7 divides evenly into -28.\n",
            "5. Calculate J = G / H, where H is 7. J = -28 / 7 = -4.\n",
            "6. Verify that J is an integer and that H * J = G. In this case, -4 is an integer and -4 * 7 = -28.\n",
            "7. Find the number on the left which is multiplied by H, and call it K. In this case, K is 7.\n",
            "8. Change K to K + J. K + J = 7 + (-4) = 3.\n",
            "9. Recompute the value on the left, and verify that it equals F. The new equation is 8 * 3 + 7 * 9 = 59.\n",
            "\n",
            "Therefore, the solution is: 8 * 3 + 7 * 9.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWvGbp7GajWf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}